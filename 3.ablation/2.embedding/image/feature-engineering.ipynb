{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Get logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "file_handler = logging.FileHandler('logs.log')\n",
    "\n",
    "# Set log format\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Settings to display log on notebook\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('/home/data/train_small')\n",
    "OUTPUT_DIR = Path(f'/home/code_for_sub/3.ablation/2.embedding/image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_behaviors = pl.read_parquet(TRAIN_DIR/'train'/'behaviors.parquet')\n",
    "trn_history = pl.read_parquet(TRAIN_DIR/'train'/'history.parquet')\n",
    "\n",
    "val_behaviors = pl.read_parquet(TRAIN_DIR/'validation'/'behaviors.parquet')\n",
    "val_history = pl.read_parquet(TRAIN_DIR/'validation'/'history.parquet')\n",
    "\n",
    "articles = pl.read_parquet(TRAIN_DIR/'articles.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Validation/Test History Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trn_history = trn_history.explode([\"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"])\n",
    "_val_history = val_history.explode([\"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"])\n",
    "\n",
    "val_history_extended = pl.concat([\n",
    "    _trn_history.filter(pl.col(\"user_id\").is_in(set(val_history[\"user_id\"]) & set(trn_history[\"user_id\"]))),\n",
    "    _val_history\n",
    "])\n",
    "val_history = val_history_extended.sort(\n",
    "    [\"user_id\", \"impression_time_fixed\"]\n",
    ").unique().groupby(\"user_id\").agg([\"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"])\n",
    "\n",
    "# val_history.write_parquet(OUTPUT_DIR/'validation'/\"history_extended.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_behaviors = pl.read_parquet(TRAIN_DIR/'train'/'behaviors.parquet')\n",
    "trn_history = pl.read_parquet(TRAIN_DIR/'train'/'history.parquet')\n",
    "\n",
    "val_behaviors = pl.read_parquet(TRAIN_DIR/'validation'/'behaviors.parquet')\n",
    "val_history = pl.read_parquet(TRAIN_DIR/'validation'/'history.parquet')\n",
    "\n",
    "articles = pl.read_parquet(TRAIN_DIR/'articles.parquet')\n",
    "\n",
    "_trn_history = trn_history.explode([\"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"])\n",
    "_val_history = val_history.explode([\"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_df(\n",
    "        df: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "    Create target_df. The target column is \"is_clicked\"\n",
    "    '''\n",
    "    \n",
    "    df = df.with_columns(\n",
    "    pl.col(\"article_ids_inview\").apply(lambda x: len(x)).alias(\"count_article_ids_inview\"),\n",
    "    )\n",
    "\n",
    "    # Explode article_ids_inview\n",
    "    df = df.explode(\"article_ids_inview\")\n",
    "\n",
    "    # If article_ids_inview is in article_ids_clicked, then 1, otherwise 0\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"article_ids_inview\").is_in(pl.col(\"article_ids_clicked\")))\n",
    "        .then(1)\n",
    "        .otherwise(0)\n",
    "        .alias(\"is_clicked\")\n",
    "    )\n",
    "\n",
    "    # Delete columns that are not used\n",
    "    df = df.drop([\"article_ids_clicked\",\"next_read_time\",\"next_scroll_percentage\",\"article_id\"])\n",
    "\n",
    "    # Change the name of article_ids_inview to article_id\n",
    "    df = df.with_columns(pl.col(\"article_ids_inview\").alias(\"article_id\"))\n",
    "    df = df.drop(\"article_ids_inview\")\n",
    "\n",
    "    # Calculate the rate of is_clicked\n",
    "    is_clicked_rate = df.select(\"is_clicked\").mean().to_pandas().iloc[0,0]\n",
    "    logger.info(f'is_clicked_rate: {is_clicked_rate}')\n",
    "\n",
    "    # Show the shape of df\n",
    "    logger.info(f'df shape: {df.shape}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inview Cooccurance Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_inview_cooccur(\n",
    "        df: pl.DataFrame, mode: str\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "    Create features that show the number of co-visits between articles.\n",
    "    '''\n",
    "\n",
    "    _len = len(df)\n",
    "\n",
    "    # Load the pre-computed co-visit counts\n",
    "    df_covisit_count = pl.read_parquet(f'/home/data/inview_cooccur/{mode}_covisit_count.parquet')\n",
    "\n",
    "    tmp_df = df.select(['impression_id','user_id','article_id'])\n",
    "\n",
    "    # Remove impression_id = 0\n",
    "    tmp_df = tmp_df.filter(pl.col('impression_id') != 0)\n",
    "\n",
    "    tmp_df = tmp_df.join(tmp_df, on = ['impression_id','user_id'],how='left')\n",
    "\n",
    "    tmp_df = tmp_df.join(df_covisit_count, on = ['article_id','article_id_right'],how='left')\n",
    "\n",
    "    feat_df = tmp_df.groupby(['impression_id','user_id','article_id']).agg(\n",
    "    pl.sum('cooccur_count').alias('cooccur_count_sum'),\n",
    "    pl.mean('cooccur_count').alias('cooccur_count_mean'),\n",
    "    pl.max('cooccur_count').alias('cooccur_count_max'),\n",
    "    pl.min('cooccur_count').alias('cooccur_count_min'),\n",
    "    pl.std('cooccur_count').alias('cooccur_count_std'),\n",
    "    )\n",
    "\n",
    "    # Normalize each feature by the maximum value for each impression_id\n",
    "    feat_df = feat_df.join(\n",
    "        feat_df.groupby('impression_id').agg(\n",
    "            pl.max('cooccur_count_sum').alias('cooccur_count_sum_max'),\n",
    "            pl.max('cooccur_count_mean').alias('cooccur_count_mean_max'),\n",
    "            pl.max('cooccur_count_max').alias('cooccur_count_max_max'),\n",
    "            pl.max('cooccur_count_min').alias('cooccur_count_min_max'),\n",
    "            pl.max('cooccur_count_std').alias('cooccur_count_std_max'),\n",
    "        ),\n",
    "        on='impression_id'\n",
    "    )\n",
    "    \n",
    "    # Normalize each feature by the maximum value for each impression_id\n",
    "    feat_df = feat_df.with_columns(\n",
    "        (pl.col('cooccur_count_sum')/pl.col('cooccur_count_sum_max')).alias('cooccur_count_sum_norm'),\n",
    "        (pl.col('cooccur_count_mean')/pl.col('cooccur_count_mean_max')).alias('cooccur_count_mean_norm'),\n",
    "        (pl.col('cooccur_count_max')/pl.col('cooccur_count_max_max')).alias('cooccur_count_max_norm'),\n",
    "        (pl.col('cooccur_count_min')/pl.col('cooccur_count_min_max')).alias('cooccur_count_min_norm'),\n",
    "        (pl.col('cooccur_count_std')/pl.col('cooccur_count_std_max')).alias('cooccur_count_std_norm'),\n",
    "    )\n",
    "\n",
    "    feat_df = feat_df.drop(['cooccur_count_sum_max','cooccur_count_mean_max','cooccur_count_max_max','cooccur_count_min_max','cooccur_count_std_max'])\n",
    "    df = df.join(feat_df, on = ['impression_id','user_id','article_id'],how='left')\n",
    "\n",
    "    assert _len == len(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Click Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_clicked_history_count(\n",
    "        df: pl.DataFrame, \n",
    "        df_history: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "    Add features that show the number of times an article has been clicked in the past.\n",
    "    '''\n",
    "    _len = len(df)\n",
    "\n",
    "    user_article = df.select(['article_id','user_id','impression_time'])\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed','scroll_percentage_fixed','article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id','impression_time_history','scroll_percentage_history','article_id','read_time_history']\n",
    "\n",
    "    # Get the combination of user_id and article_id_fixed\n",
    "    df_history = df_history.join(user_article, on=['user_id','article_id'], how='inner')\n",
    "\n",
    "    feat_df = df_history.groupby(['user_id','article_id']).agg(\n",
    "        # Count the number of times the article has been read\n",
    "        pl.count('impression_time_history').alias('article_read_count'),\n",
    "        # Get the latest time the article was read\n",
    "        pl.max('impression_time_history').alias('article_last_read_time'),\n",
    "    )\n",
    "\n",
    "    df = df.join(feat_df, on=['user_id','article_id'], how='left')\n",
    "\n",
    "    # Convert the difference between last_read_time and impression_time to X hours\n",
    "    df = df.with_columns(\n",
    "        ((pl.col('impression_time') - pl.col('article_last_read_time')) / timedelta(hours=1)).alias('article_last_read_time_diff')\n",
    "    ).drop('article_last_read_time')\n",
    "\n",
    "    # Fill the missing values in read_count, max_scroll_percentage, max_read_time with 0\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"article_read_count\").fill_null(0),\n",
    "    ])\n",
    "\n",
    "    assert _len == len(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_clicked_history_count_by_article(\n",
    "        df: pl.DataFrame, \n",
    "        df_history: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "    Add features that show the number of times an article has been clicked in the past.\n",
    "    '''\n",
    "    _len = len(df)\n",
    "\n",
    "    user_article = df.select(['article_id','impression_time'])\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed','scroll_percentage_fixed','article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id','impression_time_history','scroll_percentage_history','article_id','read_time_history']\n",
    "\n",
    "    # Get the combination of user_id and article_id_fixed\n",
    "    df_history = df_history.join(user_article, on=['article_id'], how='inner')\n",
    "\n",
    "    feat_df = df_history.groupby(['article_id']).agg(\n",
    "        # Count the number of times the article has been read\n",
    "        pl.count('impression_time_history').alias('article_read_count_v2'),\n",
    "    )\n",
    "\n",
    "    df = df.join(feat_df, on=['article_id'], how='left')\n",
    "    \n",
    "    assert _len == len(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vector Parquet Files as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-07 12:59:22,077][INFO] _vec columns : Index(['article_id', 'image_embedding'], dtype='object')\n",
      "[2024-07-07 12:59:49,615][INFO] /home/data/Ekstra_Bladet_image_embeddings/image_embeddings.parquet shape: (102603, 1025)\n"
     ]
    }
   ],
   "source": [
    "def load_vector_df(path_str: str) -> pl.DataFrame:\n",
    "    '''\n",
    "    Load the vector. The vector is provided by the original data.\n",
    "    '''\n",
    "    _vec = pd.read_parquet(path_str)\n",
    "\n",
    "    logger.info(f'_vec columns : {_vec.columns}')\n",
    "\n",
    "    col_name = _vec.columns[-1]\n",
    "\n",
    "    df_vec = _vec.apply(lambda row: pd.Series(row[col_name]), axis=1)\n",
    "    df_vec.columns = [f'vector_{i}' for i in range(df_vec.shape[1])]\n",
    "\n",
    "    df_vec['article_id'] = _vec['article_id']\n",
    "\n",
    "    df_vec = pl.from_pandas(df_vec)\n",
    "\n",
    "    logger.info(f'{path_str} shape: {df_vec.shape}')\n",
    "\n",
    "    return df_vec\n",
    "\n",
    "def load_my_vector_df(path_str: str) -> pl.DataFrame:\n",
    "    df_vec = pl.read_parquet(path_str)\n",
    "    return df_vec\n",
    "    \n",
    "\n",
    "common_vec_dict = {\n",
    "    # 'contrastive':load_vector_df('/home/data/Ekstra_Bladet_contrastive_vector/contrastive_vector.parquet'),\n",
    "    #'w2v':load_vector_df('/home/data/Ekstra_Bladet_word2vec/document_vector.parquet'),\n",
    "    #'xlm':load_vector_df('/home/data/FacebookAI_xlm_roberta_base/xlm_roberta_base.parquet'),\n",
    "    #'bert':load_vector_df('/home/data/google_bert_base_multilingual_cased/bert_base_multilingual_cased.parquet'),\n",
    "    'image':load_vector_df('/home/data/Ekstra_Bladet_image_embeddings/image_embeddings.parquet'),\n",
    "    #'bge-m3':load_my_vector_df('/home/data/bge-m3-dense/bge-m3-dense_vec_df.parquet'),\n",
    "    #'multilingual-e5-large-instruct':load_my_vector_df('/home/data/multilingual-e5-large-instruct/multilingual-e5-large-instruct_vec_df.parquet')\n",
    "}\n",
    "\n",
    "my_vec_dict = {\n",
    "    'item2vec':{\n",
    "        'train':load_my_vector_df('/home/data/item2vec_1/train_item2vec.parquet'),\n",
    "        'valid':load_my_vector_df('/home/data/item2vec_1/valid_item2vec.parquet'),\n",
    "        'test':load_my_vector_df('/home/data/item2vec_1/test_item2vec.parquet'),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cossim Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_clicked_cossim(\n",
    "        df: pl.DataFrame,\n",
    "        df_history: pl.DataFrame,\n",
    "        df_vec: pl.DataFrame,\n",
    "        cossim_name: str,\n",
    "        type: str='mean',\n",
    "        window: int=1,\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "    type:\n",
    "     mean: Cosine similarity between the average vector of the user's past actions and the article vector\n",
    "     scroll_mean: Cosine similarity between the average vector of the user's past actions and the article vector\n",
    "     read_time_mean: Cosine similarity between the average vector of the user's past actions and the article vector\n",
    "    '''\n",
    "\n",
    "    _len = len(df)\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed','scroll_percentage_fixed','article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id','impression_time_history','scroll_percentage_history','article_id','read_time_history']\n",
    "    df_history = df_history.fill_null(0)\n",
    "\n",
    "    user_article = df.select(['article_id','user_id','impression_time','impression_id'])\n",
    "\n",
    "    df_history = df_history.join(df_vec, on='article_id', how='left')\n",
    "    vec_cols = [col for col in df_history.columns if 'vector' in col]\n",
    "\n",
    "    # User embeddings\n",
    "    # How many past actions to consider with the window (if window is 0, use the entire history)\n",
    "    if window > 0:\n",
    "        df_history = df_history.sort('impression_time_history', descending=True).groupby('user_id').head(window)\n",
    "\n",
    "    if type == 'mean':\n",
    "        user_emb = df_history.groupby('user_id').agg(\n",
    "            *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "        ) # Simple mean of the latest article emb\n",
    "    elif type == 'max':\n",
    "        user_emb = df_history.groupby('user_id').agg(\n",
    "            *[pl.max(col).alias(col) for col in vec_cols]\n",
    "        ) # Simple max of the latest article emb\n",
    "    elif type == 'min':\n",
    "        user_emb = df_history.groupby('user_id').agg(\n",
    "            *[pl.min(col).alias(col) for col in vec_cols]\n",
    "        ) # Simple min of the latest article emb\n",
    "    elif type == 'scroll_mean':\n",
    "        # Scroll_percentage based mean of the latest article emb\n",
    "        for col in vec_cols:\n",
    "            df_history = df_history.with_columns(\n",
    "                (pl.col(col) * pl.col('scroll_percentage_history') / 100).alias(col)\n",
    "            )\n",
    "        user_emb = df_history.groupby('user_id').agg(\n",
    "            *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "    elif type == 'read_time_mean':\n",
    "        # Read time based mean of the latest article emb\n",
    "        user_read_time = df_history.groupby('user_id').agg(\n",
    "            pl.sum('read_time_history').alias('user_read_time')\n",
    "        )\n",
    "        df_history = df_history.join(user_read_time, on='user_id', how='left')\n",
    "        df_history = df_history.with_columns(\n",
    "            (pl.col('read_time_history') / pl.col('user_read_time')).alias('read_time_percentage')\n",
    "        )\n",
    "        for col in vec_cols:\n",
    "            df_history = df_history.with_columns(\n",
    "                (pl.col(col) * pl.col('read_time_percentage')).alias(col)\n",
    "            )\n",
    "        user_emb = df_history.groupby('user_id').agg(\n",
    "            *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "    \n",
    "    user_emb.columns = [f'user_{col}' if col != 'user_id' else col for col in user_emb.columns]\n",
    "\n",
    "    # Article embeddings\n",
    "    article_emb = df_vec.clone()\n",
    "    article_emb.columns = [f'article_{col}' if col != 'article_id' else col for col in article_emb.columns]\n",
    "\n",
    "    # Join user and article embeddings\n",
    "    user_article = user_article.join(\n",
    "    user_emb, on='user_id', how='left').join(\n",
    "        article_emb, on='article_id', how='left')\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    user_vec = user_article.select([col for col in user_article.columns if 'user_vec' in col]).to_numpy()\n",
    "    article_vec = user_article.select([col for col in user_article.columns if 'article_vec' in col]).to_numpy()\n",
    "\n",
    "    user_vec = torch.tensor(user_vec)\n",
    "    article_vec = torch.tensor(article_vec)\n",
    "\n",
    "    similarity = F.cosine_similarity(\n",
    "        user_vec, article_vec, dim=1)\n",
    "    \n",
    "    # Add as a feature\n",
    "    user_article = user_article.with_columns(\n",
    "        pl.Series(similarity.numpy()).alias(cossim_name)\n",
    "    )\n",
    "\n",
    "    feat_df = user_article.select(['user_id','article_id','impression_id',cossim_name])\n",
    "\n",
    "    df = df.join(feat_df, on=['user_id','article_id','impression_id'], how='left')\n",
    "\n",
    "    assert _len == len(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_clicked_category_cossim(\n",
    "        df: pl.DataFrame,\n",
    "        df_history: pl.DataFrame,\n",
    "        df_vec: pl.DataFrame,\n",
    "        cossim_name: str,\n",
    "        type: str='mean',\n",
    "        window: int=1,\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "    type:\n",
    "     mean: Cosine similarity between the average vector of the user's past actions and the article vector\n",
    "     scroll_mean: Cosine similarity between the average vector of the user's past actions and the article vector\n",
    "     read_time_mean: Cosine similarity between the average vector of the user's past actions and the article vector\n",
    "    '''\n",
    "    _len = len(df)\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed','scroll_percentage_fixed','article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id','impression_time_history','scroll_percentage_history','article_id','read_time_history']\n",
    "    df_history = df_history.fill_null(0)\n",
    "    df_history = df_history.join(articles.select(['article_id', 'category']), how='left', on='article_id')\n",
    "\n",
    "    user_article = df.select(['article_id','user_id','category','impression_time','impression_id'])\n",
    "\n",
    "    df_history = df_history.join(df_vec, on='article_id', how='left')\n",
    "    vec_cols = [col for col in df_history.columns if 'vector' in col]\n",
    "\n",
    "    # User embeddings\n",
    "    # How many past actions to consider with the window (if window is 0, use the entire history)\n",
    "    if window > 0:\n",
    "        df_history = df_history.sort('impression_time_history',descending=True\n",
    "                                     ).groupby(['user_id', 'category']).head(window)\n",
    "\n",
    "    if type == 'mean':\n",
    "        user_emb = df_history.groupby(['user_id', 'category']).agg(\n",
    "            *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "\n",
    "    elif type == 'min':\n",
    "        user_emb = df_history.groupby(['user_id', 'category']).agg(\n",
    "            *[pl.min(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "        \n",
    "    elif type == 'max':\n",
    "        user_emb = df_history.groupby(['user_id', 'category']).agg(\n",
    "            *[pl.max(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "\n",
    "    elif type == 'scroll_mean':\n",
    "        for col in vec_cols:\n",
    "            df_history = df_history.with_columns(\n",
    "                (pl.col(col) * pl.col('scroll_percentage_history') / 100).alias(col)\n",
    "            )\n",
    "        user_emb = df_history.groupby(['user_id', 'category']).agg(\n",
    "            *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "\n",
    "    elif type == 'read_time_mean':\n",
    "        user_read_time = df_history.groupby(['user_id', 'category']).agg(\n",
    "            pl.sum('read_time_history').alias('user_read_time')\n",
    "        )\n",
    "        df_history = df_history.join(user_read_time, on=['user_id', 'category'], how='left')\n",
    "        df_history = df_history.with_columns(\n",
    "            (pl.col('read_time_history') / pl.col('user_read_time')).alias('read_time_percentage')\n",
    "        )\n",
    "        for col in vec_cols:\n",
    "            df_history = df_history.with_columns(\n",
    "                (pl.col(col) * pl.col('read_time_percentage')).alias(col)\n",
    "            )\n",
    "\n",
    "        user_emb = df_history.groupby(['user_id', 'category']).agg(\n",
    "            *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "        )\n",
    "    \n",
    "    user_emb.columns = [f'user_{col}' if col not in ['user_id', 'category'] else col for col in user_emb.columns]\n",
    "\n",
    "\n",
    "    # Article embeddings\n",
    "    article_emb = df_vec.clone()\n",
    "    article_emb.columns = [f'article_{col}' if col != 'article_id' else col for col in article_emb.columns]\n",
    "\n",
    "    # Join user and article embeddings\n",
    "    user_article = user_article.join(\n",
    "    user_emb, on=['user_id', 'category'], how='left').join(\n",
    "        article_emb, on='article_id', how='left')\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    user_vec = user_article.select([col for col in user_article.columns if 'user_vec' in col]).to_numpy()\n",
    "    article_vec = user_article.select([col for col in user_article.columns if 'article_vec' in col]).to_numpy()\n",
    "\n",
    "    user_vec = torch.tensor(user_vec)\n",
    "    article_vec = torch.tensor(article_vec)\n",
    "\n",
    "\n",
    "    similarity = F.cosine_similarity(\n",
    "        user_vec, article_vec, dim=1)\n",
    "    \n",
    "    # Add as a feature\n",
    "    user_article = user_article.with_columns(\n",
    "        pl.Series(similarity.numpy()).alias(cossim_name)\n",
    "    )\n",
    "\n",
    "    feat_df = user_article.select(['user_id','category','article_id','impression_id',cossim_name])\n",
    "\n",
    "    df = df.join(feat_df, on=['user_id','category','article_id','impression_id'], how='left')\n",
    "\n",
    "    assert _len == len(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cossim_individual_features(\n",
    "        df: pl.DataFrame, \n",
    "        df_history: pl.DataFrame, \n",
    "        df_vec: pl.DataFrame, \n",
    "        cossim_name: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add cossim similarities between the article and the past articles.\n",
    "    \"\"\"\n",
    "\n",
    "    _len = len(df)\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed','scroll_percentage_fixed','article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id','impression_time_history','scroll_percentage_history','article_id','read_time_history']\n",
    "    df_history = df_history.fill_null(0).rename({\"article_id\": \"past_article_id\"})\n",
    "\n",
    "    user_article = df.select(['article_id','user_id','impression_time','impression_id'])\n",
    "    user_article = user_article.join(df_history.select([\"user_id\", \"past_article_id\"]), on=\"user_id\")\n",
    "\n",
    "    user_article_unique = user_article.unique([\"article_id\", \"past_article_id\"]).drop([\"user_id\", \"impression_time\", \"impression_id\"])\n",
    "\n",
    "    # Article embeddings\n",
    "    article_emb = df_vec.clone()\n",
    "    article_emb.columns = [f'article_{col}' if col != 'article_id' else col for col in article_emb.columns]\n",
    "\n",
    "    user_article_unique = user_article_unique.join(\n",
    "        article_emb, on='article_id', how='left')\n",
    "\n",
    "\n",
    "    article_emb.columns = [\"past_\" + col if col != \"article_id\" else col for col in article_emb.columns]\n",
    "\n",
    "    user_article_unique = user_article_unique.join(\n",
    "        article_emb, left_on='past_article_id', right_on=\"article_id\", how='left')\n",
    "\n",
    "    article_vec = user_article_unique.select(\n",
    "        [col for col in user_article_unique.columns if ('article_vec' in col) & (\"past_article_vec\" not in col)]\n",
    "    ).to_numpy()\n",
    "\n",
    "    past_article_vec = user_article_unique.select(\n",
    "        [col for col in user_article_unique.columns if (\"past_article_vec\" in col)]\n",
    "    ).to_numpy()\n",
    "\n",
    "    article_vec = torch.tensor(article_vec)\n",
    "    past_article_vec = torch.tensor(past_article_vec)\n",
    "\n",
    "    similarity = F.cosine_similarity(\n",
    "        article_vec, past_article_vec, dim=1)\n",
    "\n",
    "    article_similarities = pl.DataFrame(\n",
    "        {\n",
    "            \"article_id\": user_article_unique[\"article_id\"],\n",
    "            \"past_article_id\": user_article_unique[\"past_article_id\"],\n",
    "            \"cossim\": similarity.numpy()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    user_article = user_article.join(article_similarities, how=\"left\", on=[\"article_id\", \"past_article_id\"])\n",
    "\n",
    "    # Calculate features from cossim\n",
    "    features = user_article.groupby([\"impression_id\", \"article_id\"]).agg(\n",
    "        pl.col(\"cossim\").max().alias(f\"{cossim_name}_max_cossim\"),\n",
    "        pl.col(\"cossim\").min().alias(f\"{cossim_name}_min_cossim\"),\n",
    "        pl.col(\"cossim\").mean().alias(f\"{cossim_name}_mean_cossim\"),\n",
    "        pl.col(\"cossim\").median().alias(f\"{cossim_name}_median_cossim\"),\n",
    "        pl.col(\"cossim\").quantile(0.1).alias(f\"{cossim_name}_quantile_q10_cossim\"),\n",
    "        pl.col(\"cossim\").quantile(0.25).alias(f\"{cossim_name}_quantile_q25_cossim\"),\n",
    "        pl.col(\"cossim\").quantile(0.75).alias(f\"{cossim_name}_quantile_q75_cossim\"),\n",
    "        pl.col(\"cossim\").quantile(0.9).alias(f\"{cossim_name}_quantile_q90_cossim\"),\n",
    "    )\n",
    "    \n",
    "    df = df.join(features, how=\"left\", on=[\"impression_id\", \"article_id\"])\n",
    "    \n",
    "    # Normalize each feature by the maximum value for each impression_id\n",
    "    for col in [\n",
    "        f\"{cossim_name}_max_cossim\",\n",
    "        f\"{cossim_name}_min_cossim\",\n",
    "        f\"{cossim_name}_mean_cossim\",\n",
    "        f\"{cossim_name}_median_cossim\",\n",
    "        f\"{cossim_name}_quantile_q10_cossim\",\n",
    "        f\"{cossim_name}_quantile_q25_cossim\",\n",
    "        f\"{cossim_name}_quantile_q75_cossim\",\n",
    "        f\"{cossim_name}_quantile_q90_cossim\",\n",
    "    ]:\n",
    "        df = df.join(\n",
    "            df.groupby('impression_id').agg(\n",
    "                pl.max(col).alias(f'max_{col}'),\n",
    "                pl.min(col).alias(f'min_{col}'),\n",
    "            ), on='impression_id', how='left'\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(\n",
    "            ((pl.col(col) - pl.col(f'min_{col}')) / (pl.col(f'max_{col}') - pl.col(f'min_{col}'))).alias(f'{col}_norm')\n",
    "        ).drop([f'max_{col}', f'min_{col}', col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_inview_cossim(\n",
    "        df:pl.DataFrame,\n",
    "        df_vec:pl.DataFrame,\n",
    "        cossim_name:str,\n",
    ") -> pl.DataFrame:\n",
    "        '''\n",
    "        Create features based on the cosine similarity between the user's past history and the article.\n",
    "        '''\n",
    "\n",
    "        _len = len(df)\n",
    "\n",
    "        # Article embeddings\n",
    "        article_emb = df_vec.clone()\n",
    "        article_emb.columns = [f'article_{col}' if col != 'article_id' else col for col in article_emb.columns]\n",
    "\n",
    "        # User embeddings\n",
    "        df_user = df.select(['impression_id','user_id','article_id','session_id']).clone()\n",
    "        df_user = df_user.join(df_vec, on='article_id', how='left')\n",
    "        vec_cols = [col for col in df_user.columns if 'vector' in col]\n",
    "\n",
    "        for group_col in ['user_id','session_id','impression_id']:\n",
    "            user_emb = df_user.groupby(group_col).agg(\n",
    "                    *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "                    )\n",
    "            user_emb.columns = [f'user_{col}' if col != group_col else col for col in user_emb.columns]\n",
    "\n",
    "            user_article = df_user.join(\n",
    "                    user_emb, on=group_col, how='left').join(\n",
    "                    article_emb, on='article_id', how='left')\n",
    "        \n",
    "            # Calculate cosine similarities\n",
    "            user_vec = user_article.select([col for col in user_article.columns if 'user_vec' in col]).to_numpy()\n",
    "            article_vec = user_article.select([col for col in user_article.columns if 'article_vec' in col]).to_numpy()\n",
    "\n",
    "            user_vec = torch.tensor(user_vec)\n",
    "            article_vec = torch.tensor(article_vec)\n",
    "\n",
    "            similarity = F.cosine_similarity(\n",
    "                    user_vec, article_vec, dim=1)\n",
    "            \n",
    "            user_article = user_article.with_columns(\n",
    "                    pl.Series(similarity.numpy()).alias(f'{cossim_name}_{group_col}')\n",
    "                    )\n",
    "            \n",
    "            feat_df = user_article.select(['impression_id','user_id','article_id',f'{cossim_name}_{group_col}'])\n",
    "\n",
    "            df = df.join(feat_df, on=['impression_id','user_id','article_id'], how='left')\n",
    "\n",
    "            assert _len == len(df)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Article Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_article_simple(df,articles):\n",
    "    \"\"\"Add simple features from articles\"\"\"\n",
    "    _len = len(df)\n",
    "    \n",
    "    articles_numfeat = articles.select([\n",
    "    'article_id','premium','published_time','total_inviews','total_pageviews','total_read_time',\n",
    "    'sentiment_score','sentiment_label', 'image_ids'])\n",
    "    \n",
    "    articles_numfeat = articles_numfeat.with_columns(pl.col(\"image_ids\").list.lengths().alias(\"number_of_images\")).drop(\"image_ids\")\n",
    "    \n",
    "    articles_numfeat = articles_numfeat.with_columns(\n",
    "            pl.when(pl.col('sentiment_label') == 'Positive').then(pl.col('sentiment_score')).otherwise(0).alias('article_label_positive_score'),\n",
    "            pl.when(pl.col('sentiment_label') == 'Negative').then(pl.col('sentiment_score')).otherwise(0).alias('article_label_negative_score'),\n",
    "            pl.when(pl.col('sentiment_label') == 'Neutral').then(pl.col('sentiment_score')).otherwise(0).alias('article_label_neutral_score')\n",
    "    ).drop('sentiment_score','sentiment_label')\n",
    "    df = df.join(articles_numfeat, on='article_id', how='left')\n",
    "\n",
    "    # Calculate the difference between the impression time and the published time\n",
    "    df = df.with_columns(\n",
    "        ((pl.col('impression_time') - pl.col('published_time')) / timedelta(days=1)).alias('published_time_diff')\n",
    "    ).drop('published_time')\n",
    "\n",
    "    # Normalize total_inviews, total_pageviews, total_read_time by the maximum value for each impression_id\n",
    "    df = df.join(\n",
    "        df.groupby('impression_id').agg(\n",
    "            pl.max('total_inviews').alias('max_total_inviews'),\n",
    "            pl.max('total_pageviews').alias('max_total_pageviews'),\n",
    "            pl.max('total_read_time').alias('max_total_read_time')\n",
    "        ), on='impression_id', how='left'\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.col('total_inviews') / pl.col('max_total_inviews')).alias('total_inviews_norm'),\n",
    "        (pl.col('total_pageviews') / pl.col('max_total_pageviews')).alias('total_pageviews_norm'),\n",
    "        (pl.col('total_read_time') / pl.col('max_total_read_time')).alias('total_read_time_norm')\n",
    "    ).drop(['max_total_inviews','max_total_pageviews','max_total_read_time'])\n",
    "\n",
    "    # Normalize published_time_diff by the maximum value for each impression_id\n",
    "    df = df.join(\n",
    "        df.groupby('impression_id').agg(\n",
    "            pl.min('published_time_diff').alias('min_published_time_diff')\n",
    "        ), on='impression_id', how='left'\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.col('published_time_diff') / pl.col('min_published_time_diff')).alias('published_time_diff_norm')\n",
    "    ).drop('min_published_time_diff')\n",
    "\n",
    "\n",
    "    # If the user is a subscriber and the article is premium, then 1, otherwise 0\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col('premium') & pl.col('is_subscriber')).then(1).otherwise(0).alias('is_premium_subscriber')\n",
    "    )\n",
    "\n",
    "    df = df.join(articles.select([\"article_id\", \"category\"]), how=\"left\", on=\"article_id\")\n",
    "\n",
    "    assert _len == len(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Impression Time Difference Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_user_last_impression_publish_time_diff(\n",
    "    df: pl.DataFrame,\n",
    "    df_history: pl.DataFrame,\n",
    "    articles: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add the difference between the last impression time and the published time of the article.\"\"\"\n",
    "\n",
    "    shape = df.shape\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed', 'scroll_percentage_fixed', 'article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id', 'impression_time_history', 'scroll_percentage_history', 'article_id','read_time_history']\n",
    "\n",
    "    # Get the last impression time of the user\n",
    "    user_last_impression_time = df_history.groupby('user_id').agg(\n",
    "        pl.max('impression_time_history').alias('user_last_impression_time')\n",
    "    )\n",
    "\n",
    "    df = df.join(\n",
    "        user_last_impression_time, on='user_id', how='left'\n",
    "        ).join(\n",
    "            articles.select(['article_id','published_time']), on='article_id', how='left'\n",
    "        )\n",
    "\n",
    "    # Calculate the difference between the last impression time and the published time\n",
    "    df = df.with_columns(\n",
    "        ((pl.col('user_last_impression_time') - pl.col('published_time')) / timedelta(days=1)).alias('user_last_impression_time-publish_time_diff')\n",
    "    ).drop(['user_last_impression_time','published_time'])\n",
    "\n",
    "    assert df.shape[0] == shape[0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Inview Population Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_article_pop(\n",
    "    df:pl.DataFrame,\n",
    "    mode:str,\n",
    "    df_pop_path:str = '/home/data/article_pop_inview/',\n",
    "    metacol:str=None\n",
    ") -> pl.DataFrame:\n",
    "    _len = len(df)\n",
    "\n",
    "    # Load the pre-computed article popularity\n",
    "    time_interval_list = ['1m','2m','3m','5m','10m','15m','20m','30m','1h','2h','3h','6h','12h','24h'] \n",
    "\n",
    "    for time_interval in time_interval_list:\n",
    "        if not metacol:\n",
    "            file_path = f'{df_pop_path}{mode}_article_pop_inview_{time_interval}.parquet'\n",
    "        else:\n",
    "            file_path = f'{df_pop_path}{mode}_article_pop_inview_{time_interval}_{metacol}.parquet'\n",
    "\n",
    "        try:\n",
    "            df_pop = load_my_vector_df(file_path)\n",
    "        except:\n",
    "            logger.warning(f'{file_path} not found')\n",
    "            continue\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"impression_time\").dt.truncate(time_interval).alias(f'rounded_{time_interval}_datetime')\n",
    "        )\n",
    "\n",
    "        if not metacol:\n",
    "            df = df.join(df_pop, on=['article_id',f'rounded_{time_interval}_datetime'], how='left').drop(\n",
    "                [f'rounded_{time_interval}_datetime']\n",
    "            )\n",
    "        else:\n",
    "            df = df.join(df_pop, on=['article_id',f'rounded_{time_interval}_datetime',metacol], how='left').drop(\n",
    "                [f'rounded_{time_interval}_datetime']\n",
    "            )\n",
    "\n",
    "        if not metacol:\n",
    "            colname = f'rounded_{time_interval}_inview_count'\n",
    "        else:\n",
    "            colname = f'rounded_{time_interval}_inview_count_{metacol}'\n",
    "\n",
    "        df = df.join(\n",
    "            df.groupby('impression_id').agg(\n",
    "                pl.max(colname).alias(f'max_{colname}')\n",
    "            ), on='impression_id', how='left'\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(\n",
    "            (pl.col(colname) / pl.col(f'max_{colname}')).alias(f'{colname}_norm')\n",
    "        ).drop(f'max_{colname}')\n",
    "\n",
    "    assert _len == len(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Statistics of Strong Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_statistic(\n",
    "        df: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add statistics of strong features\"\"\"\n",
    "    target_cols = ['total_inviews', 'total_pageviews', 'total_read_time', 'rounded_1m_inview_count', 'rounded_2m_inview_count',\n",
    "                     'rounded_3m_inview_count','rounded_5m_inview_count','rounded_10m_inview_count','rounded_15m_inview_count','rounded_20m_inview_count',\n",
    "                     'rounded_30m_inview_count','rounded_1h_inview_count','rounded_3h_inview_count','rounded_6h_inview_count','rounded_12h_inview_count','rounded_24h_inview_count',\n",
    "                     'inview_count', 'past_inview_count', 'time_gap_to_next_inview_impression_time', 'time_gap_from_prev_inview_impression_time', 'published_time_diff'\n",
    "                    ]\n",
    "\n",
    "    operations = [\"max\", \"min\", \"mean\", \"median\", \"std\", \"skew\", \"kurtosis\"]\n",
    "\n",
    "    aggregations = []\n",
    "\n",
    "    for col in target_cols:\n",
    "        for op in operations:\n",
    "            agg_expr = getattr(pl.col(col), op)().alias(f\"{col}_{op}\")\n",
    "            aggregations.append(agg_expr)\n",
    "\n",
    "    df_agg = df.groupby(\"impression_id\").agg(aggregations)\n",
    "    df = df.join(df_agg, on=\"impression_id\", how=\"left\")\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in ['total_read_time','total_inviews', 'total_pageviews']:\n",
    "            continue\n",
    "\n",
    "        for op in [\"max\", \"mean\", \"median\", \"min\"]:\n",
    "            df = df.with_columns((pl.col(f\"{col}_{op}\") - pl.col(f\"{col}\")).alias(f\"{col}_{op}-{col}\"))\n",
    "            df = df.with_columns((pl.col(f\"{col}_{op}\") / pl.col(f\"{col}\")).alias(f\"{col}_{op}/{col}\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past Category Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_past_category_ratios(\n",
    "        df: pl.DataFrame, \n",
    "        df_history: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add past category ratios to the dataframe\"\"\"\n",
    "\n",
    "    df_history = df_history.explode(['impression_time_fixed','scroll_percentage_fixed','article_id_fixed','read_time_fixed'])\n",
    "    df_history.columns = ['user_id','impression_time_history','scroll_percentage_history','article_id','read_time_history']\n",
    "    df_history = df_history.fill_null(0)\n",
    "\n",
    "    # Joining training history data with article data\n",
    "    df_history_articles = df_history.join(articles, on='article_id', how='left')\n",
    "    user_category_df = df_history_articles.groupby([\"user_id\", \"category\"]).count().rename({\"count\": \"past_category_count\"})\n",
    "\n",
    "    user_category_df = user_category_df.with_columns(pl.col(\"past_category_count\").max().over(\"user_id\").alias(\"max_past_category_count_count\"))\n",
    "    user_category_df = user_category_df.with_columns((pl.col(\"past_category_count\") / pl.col(\"max_past_category_count_count\")).alias(\"past_category_ratio\"))\n",
    "\n",
    "\n",
    "    df = df.join(user_category_df, on=[\"user_id\", \"category\"], how=\"left\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inview Count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_inview_counts(df):\n",
    "    \"\"\"Calculate inview counts for each user_id and article_id\"\"\"\n",
    "    df = df.sort(\"impression_time\")\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"impression_id\").count().over([\"user_id\", \"article_id\"]).alias(\"inview_count\"),\n",
    "            pl.col(\"impression_id\").cumcount().over([\"user_id\", \"article_id\"]).alias(\"past_inview_count\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(\"impression_time\").shift(1).over([\"user_id\", \"article_id\"]).alias(\"next_inview_impression_time\"),\n",
    "        pl.col(\"impression_time\").shift(-1).over([\"user_id\", \"article_id\"]).alias(\"prev_inview_impression_time\")\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            ((pl.col(\"next_inview_impression_time\") - pl.col(\"impression_time\")) / timedelta(hours=1)).alias(\"time_gap_to_next_inview_impression_time\"),\n",
    "            ((pl.col(\"impression_time\") - pl.col(\"prev_inview_impression_time\")) / timedelta(hours=1)).alias(\"time_gap_from_prev_inview_impression_time\"),\n",
    "        ]\n",
    "    ).drop([\"next_inview_impression_time\", \"prev_inview_impression_time\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cossim by Category Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cossim_by_category(\n",
    "        behaviors_df: pl.DataFrame, \n",
    "        history_df: pl.DataFrame, \n",
    "        emb_df: pl.DataFrame, \n",
    "        cossim_name: str, \n",
    "        cat_col: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Cosine similarity between the user's past actions and the article vector by category.\"\"\"\n",
    "    \n",
    "    cat_vals = behaviors_df[cat_col].unique().to_list()\n",
    "    cat_weight_df = behaviors_df.select([\"user_id\", cat_col])\n",
    "\n",
    "    weight_cols = []\n",
    "    for cat_val in cat_vals:\n",
    "        cat_weight_df = cat_weight_df.with_columns((pl.col(cat_col)==cat_val).alias(f\"{cat_col}__{cat_val}\"))\n",
    "        weight_cols.append(f\"{cat_col}__{cat_val}\")\n",
    "    cat_weight_df = cat_weight_df.groupby(\"user_id\").agg([pl.col(col).mean() for col in weight_cols])\n",
    "\n",
    "    _history_df = history_df.join(\n",
    "        cat_weight_df,\n",
    "        on=\"user_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    user_emb_df = _history_df.join(\n",
    "        emb_df.rename({col: f\"userx__{col}\" for col in emb_df.columns if col != \"article_id\"}),\n",
    "        on=\"article_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    user_emb_cols = [col for col in user_emb_df.columns if \"userx__\" in col]\n",
    "\n",
    "    weighted_emb = None\n",
    "    for weight_col in weight_cols:\n",
    "        _weighted_emb = (user_emb_df.select(user_emb_cols) * user_emb_df[weight_col]).sum() / user_emb_df[weight_col].sum()\n",
    "        if weighted_emb is not None:\n",
    "            weighted_emb += cat_weight_df[weight_col] * _weighted_emb\n",
    "        else:\n",
    "            weighted_emb = cat_weight_df[weight_col] * _weighted_emb\n",
    "    weighted_emb = pl.concat([cat_weight_df.select(\"user_id\"), weighted_emb], how=\"horizontal\")\n",
    "    _behaviors_df = behaviors_df.join(weighted_emb, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    _behaviors_df = _behaviors_df.join(\n",
    "        emb_df.rename({col: f\"article__{col}\" for col in emb_df.columns if col != \"article_id\"}),\n",
    "        on=\"article_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    article_emb_cols = [col for col in _behaviors_df.columns if \"article__\" in col]\n",
    "\n",
    "    cossims = (_behaviors_df[user_emb_cols] * _behaviors_df[article_emb_cols]).sum(axis=1) / \\\n",
    "        (np.linalg.norm(_behaviors_df[user_emb_cols], axis=1) * np.linalg.norm(_behaviors_df[article_emb_cols], axis=1))\n",
    "    behaviors_df = behaviors_df.with_columns(\n",
    "        pl.lit(cossims).alias(cossim_name),\n",
    "    )\n",
    "\n",
    "    return behaviors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Rank Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rank_by_percentage(\n",
    "        df: pl.DataFrame\n",
    "    ) -> pl.DataFrame:\n",
    "    \"\"\"Normalize the rank columns by the percentage of the impression_id.\"\"\"\n",
    "    \n",
    "    rank_cols = [\n",
    "        'published_time_diff_rank',\n",
    "        'total_inviews_norm_rank',\n",
    "        'total_pageviews_norm_rank',\n",
    "        'total_read_time_norm_rank',\n",
    "        'rounded_1m_inview_count_rank',\n",
    "        'rounded_2m_inview_count_rank',\n",
    "        'rounded_3m_inview_count_rank',\n",
    "        'rounded_5m_inview_count_rank',\n",
    "        'rounded_10m_inview_count_rank',\n",
    "        'rounded_15m_inview_count_rank',\n",
    "        'rounded_20m_inview_count_rank',\n",
    "        'rounded_30m_inview_count_rank',\n",
    "        'rounded_1h_inview_count_rank',\n",
    "        'rounded_3h_inview_count_rank',\n",
    "        'rounded_6h_inview_count_rank',\n",
    "        'rounded_12h_inview_count_rank',\n",
    "        'rounded_24h_inview_count_rank',\n",
    "        'inview_count_rank',\n",
    "        'past_inview_count_rank',\n",
    "        'time_gap_to_next_inview_impression_time_rank',\n",
    "        'time_gap_from_prev_inview_impression_time_rank',\n",
    "        'next_impression_id_mean_cossim_rank',\n",
    "        'next_impression_id_min_cossim_rank',\n",
    "        'next_impression_id_max_cossim_rank'\n",
    "    ]\n",
    "\n",
    "    impression_id_value_counts = df.groupby(\"impression_id\").count().rename({\"count\": \"impression_id_article_counts\"})\n",
    "    df = df.join(impression_id_value_counts, how=\"left\", on=\"impression_id\")\n",
    "\n",
    "    expressions = [pl.col(col) / pl.col(\"impression_id_article_counts\") for col in rank_cols]\n",
    "    df = df.with_columns(expressions).drop(\"impression_id_article_counts\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Impression ID Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_impression_id_cossim(\n",
    "        df: pl.DataFrame, \n",
    "        df_vec: pl.DataFrame, \n",
    "        method: str = \"mean\"\n",
    ") -> pl.DataFrame:\n",
    "    \n",
    "    colname = f\"next_impression_id_{method}_cossim\"\n",
    "\n",
    "    df = df.sort([\"session_id\", \"impression_time\"], descending=[False, False])\n",
    "\n",
    "    tmp_df = df.select([\"impression_id\", \"session_id\", \"impression_time\"])\n",
    "    tmp_df = tmp_df.unique([\"impression_id\", \"session_id\", \"impression_time\"])\n",
    "    tmp_df = tmp_df.sort([\"session_id\", \"impression_time\"], descending=[False, False])\n",
    "    tmp_df = tmp_df.with_columns(pl.col(\"impression_id\").shift(-1).over(\"session_id\").alias(\"next_impression_id\"))\n",
    "\n",
    "    df = df.join(tmp_df.select([\"impression_id\", \"next_impression_id\"]), how=\"left\", on=\"impression_id\")\n",
    "\n",
    "    df_next_impression_id = df.filter(df[\"impression_id\"].is_in(set(df[\"next_impression_id\"]))).select([\"impression_id\", \"article_id\"])\n",
    "    df_next_impression_id = df_next_impression_id.join(df_vec, how=\"left\", on=\"article_id\")\n",
    "    \n",
    "    vec_cols = [col for col in df_next_impression_id.columns if 'vector' in col]\n",
    "    \n",
    "    if method == \"mean\":\n",
    "        next_impression_id_emb = df_next_impression_id.groupby('impression_id').agg(\n",
    "                *[pl.mean(col).alias(col) for col in vec_cols]\n",
    "                )\n",
    "    elif method == \"min\":\n",
    "        next_impression_id_emb = df_next_impression_id.groupby('impression_id').agg(\n",
    "                *[pl.min(col).alias(col) for col in vec_cols]\n",
    "                )\n",
    "    elif method == \"max\":\n",
    "        next_impression_id_emb = df_next_impression_id.groupby('impression_id').agg(\n",
    "                *[pl.max(col).alias(col) for col in vec_cols]\n",
    "                )\n",
    "    \n",
    "    next_impression_id_emb.columns = [f'next_impression_id_{col}' if col != 'impression_id' else col for col in next_impression_id_emb.columns]\n",
    "\n",
    "    df_current_impression_id = df.select([\"impression_id\", \"article_id\", \"next_impression_id\"])\n",
    "    df_current_impression_id = df_current_impression_id.join(df_vec, how=\"left\", on=\"article_id\")\n",
    "\n",
    "    df_current_impression_id.columns = [f'current_impression_id_{col}' if col not in ['article_id', 'impression_id', 'next_impression_id'] else col for col in df_current_impression_id.columns]\n",
    "\n",
    "    df_crossed = df_current_impression_id.join(next_impression_id_emb, how=\"left\", left_on=[\"next_impression_id\"], right_on=[\"impression_id\"])\n",
    "    df_crossed = df_crossed.drop_nulls([\"next_impression_id\"])\n",
    "\n",
    "    current_impression_id_vec = df_crossed.select([col for col in df_crossed.columns if 'current_impression_id_' in col]).to_numpy()\n",
    "    next_impression_id_vec = df_crossed.select([col for col in df_crossed.columns if 'next_impression_id_' in col]).to_numpy()\n",
    "\n",
    "    current_impression_id_vec = torch.tensor(current_impression_id_vec)\n",
    "    next_impression_id_vec = torch.tensor(next_impression_id_vec)\n",
    "\n",
    "    similarity = F.cosine_similarity(\n",
    "            current_impression_id_vec, next_impression_id_vec, dim=1)\n",
    "\n",
    "    df_crossed = df_crossed.with_columns(\n",
    "            pl.Series(similarity.numpy()).alias(colname)\n",
    "            )\n",
    "\n",
    "    feat_df = df_crossed.select(['impression_id','article_id',colname])\n",
    "    df = df.join(feat_df, how=\"left\", on=[\"impression_id\", \"article_id\"]).drop(\"next_impression_id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Time Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_published_time_features(\n",
    "        df: pl.DataFrame, \n",
    "        df_history: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add features related to the published time of the articles.\"\"\"\n",
    "    _df_history = df_history.explode(\n",
    "        [\n",
    "            \"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"\n",
    "        ]\n",
    "    ).rename({\n",
    "        \"impression_time_fixed\": \"impression_time\", \n",
    "        \"scroll_percentage_fixed\": \"scroll_percentage\", \n",
    "        \"article_id_fixed\": \"article_id\", \n",
    "        \"read_time_fixed\": \"read_time\"\n",
    "    })\n",
    "\n",
    "    _df_history = _df_history.join(articles, how=\"left\", on=\"article_id\")\n",
    "    _df_history = _df_history.with_columns(\n",
    "            ((pl.col('impression_time') - pl.col('published_time')) / timedelta(days=1)).alias('published_time_diff')\n",
    "    )\n",
    "    _df_history = _df_history.groupby([\"user_id\"]).agg([\n",
    "        pl.col(\"published_time_diff\").mean().alias(\"mean_history_published_time_diff\"),\n",
    "        pl.col(\"published_time_diff\").max().alias(\"max_history_published_time_diff\"),\n",
    "        pl.col(\"published_time_diff\").min().alias(\"min_history_published_time_diff\"),\n",
    "        pl.col(\"published_time_diff\").median().alias(\"median_history_published_time_diff\"),\n",
    "    ])\n",
    "    \n",
    "    df = df.join(_df_history, how=\"left\", on=\"user_id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(\n",
    "        df: pl.DataFrame,\n",
    "        df_history: pl.DataFrame,\n",
    "        mode: str = 'train',\n",
    "        logging: bool = True\n",
    ") -> pl.DataFrame:\n",
    "    \n",
    "    if logging:\n",
    "        logger.info('start:feature_engineering')\n",
    "        logger.info(f'df shape: {df.shape}')\n",
    "        current_time = datetime.now()\n",
    "                \n",
    "    df = feat_clicked_history_count(df,df_history)\n",
    "    df = feat_clicked_history_count_by_article(df,df_history)\n",
    "\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'clicked_history_count/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    df = feat_inview_cooccur(df,mode)\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'inview_cooccur/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    df = feat_article_simple(df,articles)\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'feat_article_simple/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    df = feat_user_last_impression_publish_time_diff(df,df_history,articles)\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'user_last_impression_publish_time_diff/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "                    \n",
    "    df = feat_article_pop(df,mode,df_pop_path='/home/data/article_pop_inview/',metacol=None)\n",
    "    \n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'article_pop/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    df = add_past_category_ratios(df,df_history)\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'add_past_category_ratios/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "        \n",
    "    df = add_next_impression_id_cossim(df, common_vec_dict[\"image\"], method=\"mean\")\n",
    "    df = add_next_impression_id_cossim(df, common_vec_dict[\"image\"], method=\"min\")\n",
    "    df = add_next_impression_id_cossim(df, common_vec_dict[\"image\"], method=\"max\")\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'add_next_impression_id_cossim/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    for k,v in common_vec_dict.items():\n",
    "        for type in ['mean','scroll_mean','read_time_mean']:#'min', 'max', \n",
    "            for window in [1,0]:\n",
    "                if window == 1 and type != 'mean':\n",
    "                    continue\n",
    "\n",
    "                df = feat_clicked_cossim(df,df_history,v,f'click_cossim_{k}_{type}_{window}',type,window)\n",
    "            \n",
    "            if logging:\n",
    "                logger.info(f'k: {k}, type: {type}')\n",
    "                elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "                current_time = datetime.now()\n",
    "                logger.info(f'clicked_inview_common_cossim/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "                \n",
    "            df = feat_clicked_category_cossim(df,df_history,v,f'category_click_cossim_{k}_{type}',type,window=0)\n",
    "            if logging:\n",
    "                logger.info(f'k: {k}, type: {type}')\n",
    "                elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "                current_time = datetime.now()\n",
    "                logger.info(f'category_click_cossim/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "        df = feat_inview_cossim(df,v,f'inview_cossim_{k}')\n",
    "\n",
    "        if logging:\n",
    "            logger.info(f'k: {k}')\n",
    "            elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "            current_time = datetime.now()\n",
    "            logger.info(f'inview_my_cossim/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "        df = add_cossim_individual_features(df, df_history, v, f\"{k}_individual\")\n",
    "        if logging:\n",
    "            logger.info(f'k: {k}')\n",
    "            elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "            current_time = datetime.now()\n",
    "            logger.info(f'add_cossim_individual_features/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "    \n",
    "\n",
    "    for k,v in my_vec_dict.items():\n",
    "        for type in ['mean','scroll_mean','read_time_mean']:\n",
    "            for window in [1,2,3,0]:\n",
    "                if window == 1 and type != 'mean':\n",
    "                    continue\n",
    "                #click\n",
    "                df = feat_clicked_cossim(df,df_history,v[mode],f'click_cossim_{k}_{type}_{window}',type,window)\n",
    "\n",
    "                if logging:\n",
    "                    logger.info(f'k: {k}, type: {type}')\n",
    "                    elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "                    current_time = datetime.now()\n",
    "                    logger.info(f'clicked_inview_my_cossim/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    cat_cols = [\"device_type\", \"is_subscriber\", \"is_sso_user\"]\n",
    "    for k,v in common_vec_dict.items():\n",
    "        if k == 'one_hot':\n",
    "            continue\n",
    "        for cat_col in cat_cols:\n",
    "            df = add_cossim_by_category(\n",
    "                df,\n",
    "                df_history.rename(\n",
    "                    {\"impression_time_fixed\": \"impression_time\",\n",
    "                     \"scroll_percentage_fixed\": \"scroll_percentage\",\n",
    "                     \"article_id_fixed\": \"article_id\",\n",
    "                     \"read_time_fixed\": \"read_time\"}\n",
    "                ).explode(\n",
    "                    [\"impression_time\", \"scroll_percentage\", \"article_id\", \"read_time\"]\n",
    "                ).sort(\n",
    "                    [\"user_id\", \"impression_time\"]\n",
    "                ),\n",
    "                v,\n",
    "                f\"cossim__{k}__{cat_col}\", cat_col)\n",
    "        \n",
    "            if logging:\n",
    "                logger.info(f'k: {k}, cat_col: {cat_col}')\n",
    "                elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "                current_time = datetime.now()\n",
    "                logger.info(f'add_cossim_by_category/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "        \n",
    "    df = calculate_inview_counts(df)\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'calculate_inview_counts/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == pl.Boolean:\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            \n",
    "    df = add_statistic(df)\n",
    "    if logging:\n",
    "        elapsed_seconds = (datetime.now() - current_time).seconds\n",
    "        current_time = datetime.now()\n",
    "        logger.info(f'statistic/elapsed_seconds: {elapsed_seconds}/df shape: {df.shape}')\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        (pl.col('total_pageviews') / pl.col('total_inviews')).alias('pageviews/inviews'),\n",
    "        (pl.col('total_read_time') / pl.col('total_inviews')).alias('read_time/inviews'),\n",
    "        pl.col('published_time_diff').rank().over('impression_id').alias('published_time_diff_rank'),\n",
    "    ])\n",
    "\n",
    "    rank_cols = ['total_inviews_norm', 'total_pageviews_norm', 'total_read_time_norm', 'rounded_1m_inview_count', 'rounded_2m_inview_count',\n",
    "                 'rounded_3m_inview_count','rounded_5m_inview_count','rounded_10m_inview_count','rounded_15m_inview_count','rounded_20m_inview_count',\n",
    "                 'rounded_30m_inview_count','rounded_1h_inview_count','rounded_3h_inview_count','rounded_6h_inview_count','rounded_12h_inview_count','rounded_24h_inview_count',\n",
    "                 'inview_count', 'past_inview_count', 'time_gap_to_next_inview_impression_time', 'time_gap_from_prev_inview_impression_time', \n",
    "                 'next_impression_id_mean_cossim', 'next_impression_id_min_cossim', 'next_impression_id_max_cossim', \n",
    "                 'published_time_diff','published_time_diff_min-published_time_diff'\n",
    "                ]\n",
    "    for col in rank_cols:\n",
    "        df = df.with_columns(pl.col(col).rank(descending =True).over('impression_id').alias(f'{col}_rank'))\n",
    "\n",
    "    df = df.with_columns([\n",
    "        df['impression_time'].dt.hour().alias('hour'),\n",
    "        df['impression_time'].dt.minute().alias('minute'),\n",
    "        df['impression_time'].dt.second().alias('second'),\n",
    "        df['impression_time'].dt.weekday().alias('weekday')\n",
    "    ])    \n",
    "    \n",
    "    df = normalize_rank_by_percentage(df)\n",
    "    \n",
    "    grouped_df = df.groupby(\"article_id\").agg(\n",
    "        [\n",
    "            pl.col(\"count_article_ids_inview\").mean().alias(\"mean_count_article_ids_inview\"),\n",
    "            pl.col(\"count_article_ids_inview\").min().alias(\"min_count_article_ids_inview\"),\n",
    "            pl.col(\"count_article_ids_inview\").median().alias(\"median_count_article_ids_inview\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.join(grouped_df, on=[\"article_id\"], how=\"left\")\n",
    "    \n",
    "    df = add_published_time_features(df, df_history)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-07 12:59:53,667][INFO] is_clicked_rate: 0.09040443631956259\n",
      "[2024-07-07 12:59:53,669][INFO] df shape: (2585747, 15)\n",
      "[2024-07-07 12:59:53,671][INFO] start:feature_engineering\n",
      "[2024-07-07 12:59:53,672][INFO] df shape: (2585747, 15)\n",
      "[2024-07-07 13:00:02,998][INFO] clicked_history_count/elapsed_seconds: 9/df shape: (2585747, 18)\n",
      "[2024-07-07 13:00:09,560][INFO] inview_cooccur/elapsed_seconds: 6/df shape: (2585747, 28)\n",
      "[2024-07-07 13:00:10,796][INFO] feat_article_simple/elapsed_seconds: 1/df shape: (2585747, 43)\n",
      "[2024-07-07 13:00:11,068][INFO] user_last_impression_publish_time_diff/elapsed_seconds: 0/df shape: (2585747, 44)\n",
      "[2024-07-07 13:00:20,250][INFO] article_pop/elapsed_seconds: 9/df shape: (2585747, 72)\n",
      "[2024-07-07 13:00:41,741][INFO] add_past_category_ratios/elapsed_seconds: 21/df shape: (2585747, 75)\n",
      "[2024-07-07 13:01:42,287][INFO] add_next_impression_id_cossim/elapsed_seconds: 60/df shape: (2585747, 78)\n",
      "[2024-07-07 13:02:37,024][INFO] k: image, type: mean\n",
      "[2024-07-07 13:02:37,053][INFO] clicked_inview_common_cossim/elapsed_seconds: 54/df shape: (2585747, 80)\n",
      "[2024-07-07 13:03:02,963][INFO] k: image, type: mean\n",
      "[2024-07-07 13:03:02,966][INFO] category_click_cossim/elapsed_seconds: 25/df shape: (2585747, 81)\n",
      "[2024-07-07 13:04:10,188][INFO] k: image, type: scroll_mean\n",
      "[2024-07-07 13:04:10,192][INFO] clicked_inview_common_cossim/elapsed_seconds: 67/df shape: (2585747, 82)\n",
      "[2024-07-07 13:05:17,064][INFO] k: image, type: scroll_mean\n",
      "[2024-07-07 13:05:17,069][INFO] category_click_cossim/elapsed_seconds: 66/df shape: (2585747, 83)\n",
      "[2024-07-07 13:06:00,101][INFO] k: image, type: read_time_mean\n",
      "[2024-07-07 13:06:00,104][INFO] clicked_inview_common_cossim/elapsed_seconds: 43/df shape: (2585747, 84)\n",
      "[2024-07-07 13:06:44,323][INFO] k: image, type: read_time_mean\n",
      "[2024-07-07 13:06:44,327][INFO] category_click_cossim/elapsed_seconds: 44/df shape: (2585747, 85)\n",
      "[2024-07-07 13:08:30,505][INFO] k: image\n",
      "[2024-07-07 13:08:30,513][INFO] inview_my_cossim/elapsed_seconds: 106/df shape: (2585747, 88)\n",
      "[2024-07-07 13:11:13,805][INFO] k: image\n",
      "[2024-07-07 13:11:13,813][INFO] add_cossim_individual_features/elapsed_seconds: 163/df shape: (2585747, 96)\n",
      "[2024-07-07 13:11:18,229][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:11:18,232][INFO] clicked_inview_my_cossim/elapsed_seconds: 4/df shape: (2585747, 97)\n",
      "[2024-07-07 13:11:21,303][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:11:21,306][INFO] clicked_inview_my_cossim/elapsed_seconds: 3/df shape: (2585747, 98)\n",
      "[2024-07-07 13:11:24,346][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:11:24,348][INFO] clicked_inview_my_cossim/elapsed_seconds: 3/df shape: (2585747, 99)\n",
      "[2024-07-07 13:11:26,466][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:11:26,467][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2585747, 100)\n",
      "[2024-07-07 13:11:29,036][INFO] k: item2vec, type: scroll_mean\n",
      "[2024-07-07 13:11:29,037][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2585747, 101)\n",
      "[2024-07-07 13:11:32,035][INFO] k: item2vec, type: scroll_mean\n",
      "[2024-07-07 13:11:32,036][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2585747, 102)\n",
      "[2024-07-07 13:11:38,000][INFO] k: item2vec, type: scroll_mean\n",
      "[2024-07-07 13:11:38,001][INFO] clicked_inview_my_cossim/elapsed_seconds: 5/df shape: (2585747, 103)\n",
      "[2024-07-07 13:11:42,481][INFO] k: item2vec, type: read_time_mean\n",
      "[2024-07-07 13:11:42,485][INFO] clicked_inview_my_cossim/elapsed_seconds: 4/df shape: (2585747, 104)\n",
      "[2024-07-07 13:11:45,036][INFO] k: item2vec, type: read_time_mean\n",
      "[2024-07-07 13:11:45,037][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2585747, 105)\n",
      "[2024-07-07 13:11:49,588][INFO] k: item2vec, type: read_time_mean\n",
      "[2024-07-07 13:11:49,590][INFO] clicked_inview_my_cossim/elapsed_seconds: 4/df shape: (2585747, 106)\n",
      "[2024-07-07 13:13:06,614][INFO] k: image, cat_col: device_type\n",
      "[2024-07-07 13:13:06,631][INFO] add_cossim_by_category/elapsed_seconds: 77/df shape: (2585747, 107)\n",
      "[2024-07-07 13:14:22,788][INFO] k: image, cat_col: is_subscriber\n",
      "[2024-07-07 13:14:22,790][INFO] add_cossim_by_category/elapsed_seconds: 76/df shape: (2585747, 108)\n",
      "[2024-07-07 13:15:27,560][INFO] k: image, cat_col: is_sso_user\n",
      "[2024-07-07 13:15:27,563][INFO] add_cossim_by_category/elapsed_seconds: 64/df shape: (2585747, 109)\n",
      "[2024-07-07 13:15:36,857][INFO] calculate_inview_counts/elapsed_seconds: 9/df shape: (2585747, 113)\n",
      "[2024-07-07 13:15:48,783][INFO] statistic/elapsed_seconds: 11/df shape: (2585747, 404)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR/'train', exist_ok=True)\n",
    "\n",
    "trn_df = get_target_df(trn_behaviors)\n",
    "trn_df = create_feature(trn_df, trn_history, mode='train')\n",
    "\n",
    "trn_df.write_parquet(OUTPUT_DIR / 'train' / 'trn_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-07 13:17:44,189][INFO] is_clicked_rate: 0.08386031543130591\n",
      "[2024-07-07 13:17:44,191][INFO] df shape: (2928942, 15)\n",
      "[2024-07-07 13:17:44,192][INFO] start:feature_engineering\n",
      "[2024-07-07 13:17:44,193][INFO] df shape: (2928942, 15)\n",
      "[2024-07-07 13:17:53,529][INFO] clicked_history_count/elapsed_seconds: 9/df shape: (2928942, 18)\n",
      "[2024-07-07 13:18:00,595][INFO] inview_cooccur/elapsed_seconds: 7/df shape: (2928942, 28)\n",
      "[2024-07-07 13:18:01,335][INFO] feat_article_simple/elapsed_seconds: 0/df shape: (2928942, 43)\n",
      "[2024-07-07 13:18:01,621][INFO] user_last_impression_publish_time_diff/elapsed_seconds: 0/df shape: (2928942, 44)\n",
      "[2024-07-07 13:18:15,447][INFO] article_pop/elapsed_seconds: 13/df shape: (2928942, 72)\n",
      "[2024-07-07 13:18:28,020][INFO] add_past_category_ratios/elapsed_seconds: 12/df shape: (2928942, 75)\n",
      "[2024-07-07 13:19:26,610][INFO] add_next_impression_id_cossim/elapsed_seconds: 58/df shape: (2928942, 78)\n",
      "[2024-07-07 13:20:29,006][INFO] k: image, type: mean\n",
      "[2024-07-07 13:20:29,015][INFO] clicked_inview_common_cossim/elapsed_seconds: 62/df shape: (2928942, 80)\n",
      "[2024-07-07 13:20:53,767][INFO] k: image, type: mean\n",
      "[2024-07-07 13:20:53,768][INFO] category_click_cossim/elapsed_seconds: 24/df shape: (2928942, 81)\n",
      "[2024-07-07 13:21:40,142][INFO] k: image, type: scroll_mean\n",
      "[2024-07-07 13:21:40,143][INFO] clicked_inview_common_cossim/elapsed_seconds: 46/df shape: (2928942, 82)\n",
      "[2024-07-07 13:22:25,815][INFO] k: image, type: scroll_mean\n",
      "[2024-07-07 13:22:25,819][INFO] category_click_cossim/elapsed_seconds: 45/df shape: (2928942, 83)\n",
      "[2024-07-07 13:22:59,518][INFO] k: image, type: read_time_mean\n",
      "[2024-07-07 13:22:59,519][INFO] clicked_inview_common_cossim/elapsed_seconds: 33/df shape: (2928942, 84)\n",
      "[2024-07-07 13:23:35,172][INFO] k: image, type: read_time_mean\n",
      "[2024-07-07 13:23:35,173][INFO] category_click_cossim/elapsed_seconds: 35/df shape: (2928942, 85)\n",
      "[2024-07-07 13:25:04,897][INFO] k: image\n",
      "[2024-07-07 13:25:04,899][INFO] inview_my_cossim/elapsed_seconds: 89/df shape: (2928942, 88)\n",
      "[2024-07-07 13:26:56,177][INFO] k: image\n",
      "[2024-07-07 13:26:56,178][INFO] add_cossim_individual_features/elapsed_seconds: 111/df shape: (2928942, 96)\n",
      "[2024-07-07 13:26:58,778][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:26:58,779][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2928942, 97)\n",
      "[2024-07-07 13:27:01,615][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:27:01,618][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2928942, 98)\n",
      "[2024-07-07 13:27:04,612][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:27:04,613][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2928942, 99)\n",
      "[2024-07-07 13:27:08,123][INFO] k: item2vec, type: mean\n",
      "[2024-07-07 13:27:08,124][INFO] clicked_inview_my_cossim/elapsed_seconds: 3/df shape: (2928942, 100)\n",
      "[2024-07-07 13:27:11,625][INFO] k: item2vec, type: scroll_mean\n",
      "[2024-07-07 13:27:11,626][INFO] clicked_inview_my_cossim/elapsed_seconds: 3/df shape: (2928942, 101)\n",
      "[2024-07-07 13:27:14,185][INFO] k: item2vec, type: scroll_mean\n",
      "[2024-07-07 13:27:14,189][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2928942, 102)\n",
      "[2024-07-07 13:27:18,922][INFO] k: item2vec, type: scroll_mean\n",
      "[2024-07-07 13:27:18,924][INFO] clicked_inview_my_cossim/elapsed_seconds: 4/df shape: (2928942, 103)\n",
      "[2024-07-07 13:27:23,923][INFO] k: item2vec, type: read_time_mean\n",
      "[2024-07-07 13:27:23,925][INFO] clicked_inview_my_cossim/elapsed_seconds: 5/df shape: (2928942, 104)\n",
      "[2024-07-07 13:27:25,945][INFO] k: item2vec, type: read_time_mean\n",
      "[2024-07-07 13:27:25,947][INFO] clicked_inview_my_cossim/elapsed_seconds: 2/df shape: (2928942, 105)\n",
      "[2024-07-07 13:27:29,085][INFO] k: item2vec, type: read_time_mean\n",
      "[2024-07-07 13:27:29,086][INFO] clicked_inview_my_cossim/elapsed_seconds: 3/df shape: (2928942, 106)\n",
      "[2024-07-07 13:28:27,375][INFO] k: image, cat_col: device_type\n",
      "[2024-07-07 13:28:27,376][INFO] add_cossim_by_category/elapsed_seconds: 58/df shape: (2928942, 107)\n",
      "[2024-07-07 13:29:18,088][INFO] k: image, cat_col: is_subscriber\n",
      "[2024-07-07 13:29:18,092][INFO] add_cossim_by_category/elapsed_seconds: 50/df shape: (2928942, 108)\n",
      "[2024-07-07 13:30:04,477][INFO] k: image, cat_col: is_sso_user\n",
      "[2024-07-07 13:30:04,479][INFO] add_cossim_by_category/elapsed_seconds: 46/df shape: (2928942, 109)\n",
      "[2024-07-07 13:30:15,465][INFO] calculate_inview_counts/elapsed_seconds: 10/df shape: (2928942, 113)\n",
      "[2024-07-07 13:30:21,553][INFO] statistic/elapsed_seconds: 6/df shape: (2928942, 404)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR/'validation', exist_ok=True)\n",
    "\n",
    "val_df = get_target_df(val_behaviors)\n",
    "val_df = create_feature(val_df, val_history, mode='valid')\n",
    "\n",
    "val_df.write_parquet(OUTPUT_DIR / 'validation' / 'val_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
